{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "complimentary-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import bs4\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-warning",
   "metadata": {},
   "source": [
    "# UK Enterococcus Paper Metadata\n",
    "\n",
    "## Add sample and project accessions to UK metadata\n",
    "\n",
    "Starting from table S2 in https://mbio.asm.org/content/9/6/e01780-18 which contains `run_accessions` for deposited raw sequencing data in ENA, use the ENA API to automatically add `study_accession` and `sample_accession`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "multiple-roberts",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e63f749a06a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0muk_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muk_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madd_study_and_sample_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_response_soup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/eda/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7763\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7764\u001b[0m         )\n\u001b[0;32m-> 7765\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/eda/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/eda/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/eda/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e63f749a06a6>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0muk_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muk_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madd_study_and_sample_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_response_soup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-e63f749a06a6>\u001b[0m in \u001b[0;36madd_study_and_sample_metadata\u001b[0;34m(row, api_response_soup)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mrun_accession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Run_accession'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mrun_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_response_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_accession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_soup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mxref\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xref_link'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/eda/lib/python3.8/site-packages/bs4/element.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, name, attrs, recursive, text, **kwargs)\u001b[0m\n\u001b[1;32m   1759\u001b[0m         \"\"\"\n\u001b[1;32m   1760\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1761\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/eda/lib/python3.8/site-packages/bs4/element.py\u001b[0m in \u001b[0;36mfind_all\u001b[0;34m(self, name, attrs, recursive, text, limit, **kwargs)\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m             \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1788\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1789\u001b[0m     \u001b[0mfindAll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_all\u001b[0m       \u001b[0;31m# BS3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m     \u001b[0mfindChildren\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_all\u001b[0m  \u001b[0;31m# BS2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/eda/lib/python3.8/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m_find_all\u001b[0;34m(self, name, attrs, text, limit, generator, **kwargs)\u001b[0m\n\u001b[1;32m    781\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/eda/lib/python3.8/site-packages/bs4/element.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m   2063\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2065\u001b[0;31m                 \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2066\u001b[0m         \u001b[0;31m# If it's text, make sure the text matches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2067\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNavigableString\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/eda/lib/python3.8/site-packages/bs4/element.py\u001b[0m in \u001b[0;36msearch_tag\u001b[0;34m(self, markup_name, markup_attrs)\u001b[0m\n\u001b[1;32m   2005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m         call_function_with_tag_data = (\n\u001b[0;32m-> 2007\u001b[0;31m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m             and not isinstance(markup_name, Tag))\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/eda/lib/python3.8/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# table S2 from https://mbio.asm.org/content/9/6/e01780-18\n",
    "uk_metadata = pd.read_csv('datasets/inline-supplementary-material-3.csv', sep='\\t', skiprows=1)\n",
    "uk_metadata = uk_metadata.rename(columns={'Accession number': 'Run_accession', \n",
    "                                          'Isolate ID': 'Isolate_name',\n",
    "                                          'ST': 'Sequence_Type'})\n",
    "\n",
    "# break into 500 accession chunks to more efficiently query the API\n",
    "api_responses = \"\"\n",
    "\n",
    "run_accessions = uk_metadata.loc[uk_metadata['Run_accession'].str.startswith('ERR'), 'Run_accession'].values\n",
    "for run_accession_chunk in [run_accessions[i:i + 500] for i in range(0, len(run_accessions), 500)]:\n",
    "    run_accession_chunk = \",\".join(run_accession_chunk)\n",
    "    api_response = requests.get(f\"https://www.ebi.ac.uk/ena/browser/api/xml/{run_accession_chunk}\")    \n",
    "    api_responses += api_response.text\n",
    "\n",
    "# parse combined xml records into a soup for easier traversal \n",
    "api_response_soup = bs4.BeautifulSoup(api_responses, 'lxml')\n",
    "\n",
    "\n",
    "def add_study_and_sample_metadata(row, api_response_soup):\n",
    "    \"\"\"\n",
    "    Use the ENA API to get the study and sample accessions\n",
    "    \"\"\"\n",
    "    run_accession = row['Run_accession']\n",
    "        \n",
    "    run_soup = api_response_soup.find(accession=run_accession)\n",
    "    if run_soup:\n",
    "        for xref in run_soup.find_all('xref_link'):\n",
    "            if xref.db.text == 'ENA-STUDY':\n",
    "                row['Study_accession'] = xref.id.text.strip()\n",
    "            elif xref.db.text == 'ENA-SAMPLE':\n",
    "                row['Sample_accession'] = xref.id.text.strip()\n",
    "    else:\n",
    "        row['Study_accession'] = np.nan\n",
    "        row['Sample_accession'] = np.nan\n",
    "    return row\n",
    "\n",
    "uk_metadata = uk_metadata.apply(lambda x: add_study_and_sample_metadata(x, api_response_soup), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-shell",
   "metadata": {},
   "source": [
    "Then let's tidy up the UK metadata sheet by renaming some fields and adding some extra metadata to make our life easier merging in with the alberta data later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy up UK metadata for merging\n",
    "uk_metadata = uk_metadata.rename(columns={'Origin': 'Origin',\n",
    "                                          'BAPS group': 'BAPS_group',\n",
    "                                          'Location': 'Location',\n",
    "                                          'Ampicillin resistance': 'Ampicillin',\n",
    "                                          'Vancomycin resistance': 'Vancomycin'})\n",
    "\n",
    "uk_metadata.loc[uk_metadata['Removed in deduplication'] == 'Removed', 'Metadata_status'] = 'Removed for deduplication in original paper (10.1128/mBio.01780-18)'\n",
    "uk_metadata = uk_metadata.drop('Removed in deduplication', axis=1)\n",
    "\n",
    "# all in this paper are E. faecium\n",
    "uk_metadata['Species'] = 'Enterococcus faecium'\n",
    "uk_metadata['Country/Province'] = 'United Kingdom'\n",
    "\n",
    "# drop reference strains\n",
    "uk_metadata = uk_metadata.loc[uk_metadata['Origin'] != 'Reference strain']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-morning",
   "metadata": {},
   "source": [
    "## Add read data for UK reads to the UK metadata\n",
    "\n",
    "Then parse the read data for all the reads (or accessions) given to me by HS and archived on `deivos.research.cs.dal.ca`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data = pd.read_csv('Enterococcus_reads.tsv', sep='\\t')\n",
    "read_data = read_data.rename(columns={'Sample_name': \"Strain_name\"})\n",
    "read_data = read_data.replace({'E_faecalis': 'Enterococcus faecalis',\n",
    "                               'E_faecium': 'Enterococcus faecium'})\n",
    "\n",
    "# split the datasets for now\n",
    "uk_read_data = read_data[read_data['Strain_name'].str.startswith('ERR')]\n",
    "uk_read_data['Run_accession'] = uk_read_data['Strain_name']\n",
    "alberta_read_data = read_data[~read_data['Strain_name'].str.startswith('ERR')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-freedom",
   "metadata": {},
   "source": [
    "And merge the UK metadata and UK read data using the `Run_accession` ensuring injective mapping.\n",
    "\n",
    "Add explanation about why readsets are missing, download protocol initially used by HS:\n",
    "1. get all accessions\n",
    "2. try to download\n",
    "3. keep the subset that successfully downloaded as a \"random sample with similar habitat distribution as all Alberta E. faecium metadata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_metadata_and_reads = pd.merge(uk_metadata, \n",
    "                                 uk_read_data, \n",
    "                                 on='Run_accession', \n",
    "                                 validate='one_to_one', \n",
    "                                 suffixes=['_metadata', '_reads'],\n",
    "                                 how='outer')\n",
    "uk_metadata_and_reads.loc[uk_metadata_and_reads['Read_1'].isna(), 'Read_status'] = \"Not successfully downloaded in initial attempt by HS\"\n",
    "\n",
    "# drop extraneous columns\n",
    "uk_metadata_and_reads = uk_metadata_and_reads.drop(['Strain_name', 'Species_reads'], axis=1)\n",
    "\n",
    "# make species information from metadata the official species for the UK data as this seems reliable for this dataset\n",
    "uk_metadata_and_reads = uk_metadata_and_reads.rename(columns={'Species_metadata': 'Species'})\n",
    "\n",
    "# re-order columns into natural groupings\n",
    "uk_metadata_and_reads = uk_metadata_and_reads[\n",
    "                      ['Study_accession', 'Sample_accession', \n",
    "                       'Run_accession', 'Isolate_name', \n",
    "                       'Metadata_status',\n",
    "                       'Species', 'BAPS_group', 'Sequence_Type',\n",
    "                       'Country/Province', 'Origin', 'Location',\n",
    "                      'Ampicillin', 'Vancomycin', 'Read_status', \n",
    "                      'Read_1', \"Read_2\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-corpus",
   "metadata": {},
   "source": [
    "# AAFC Enterococcus Paper Metadata\n",
    "\n",
    "## Link NCBI metadata to metadata published with paper\n",
    "\n",
    "https://www.nature.com/articles/s41598-020-61002-5\n",
    "\n",
    "Raw sequencing data was not deposited for this paper so only a `BioProject` accession (`PRJNA604849`) i.e., `study_accession` above was provided, this study on NCBI contains `sample_accessions` and the raw assembly files.\n",
    "\n",
    "Therefore, download the metadata from this study and tidy up column names to make merging easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/PRJNA604849_full_biosample_list.xml') as fh:\n",
    "    PRJNA604849_xml = bs4.BeautifulSoup(fh, 'lxml')\n",
    "\n",
    "parsed_xml_data = {}\n",
    "for biosample in PRJNA604849_xml.find_all('biosample'):\n",
    "    biosample_data = {}\n",
    "    biosample_data['Sample_name'] = biosample.find(db_label='Sample name').text\n",
    "    biosample_data['Species'] = biosample.find('organismname').text\n",
    "    biosample_data['Strain_name'] = biosample.find(attribute_name=\"strain\").text\n",
    "    biosample_data['Study_accession'] = biosample.find(target=\"bioproject\")['label']\n",
    "    parsed_xml_data[biosample['accession']] = biosample_data\n",
    "    \n",
    "alberta_ncbi = pd.DataFrame(parsed_xml_data).T.reset_index().rename(columns={'index': 'Sample_accession'})\n",
    "\n",
    "# tidy up as we don't use sample_name for matching because strain_name was a closer match to the isolate name\n",
    "# in the paper metadata\n",
    "alberta_ncbi = alberta_ncbi.drop('Sample_name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-television",
   "metadata": {},
   "source": [
    "Now we need to parse and tidy up the metadata from https://www.nature.com/articles/s41598-020-61002-5  `41598_2020_61002_MOESM2_ESM.csv` as this seems to have some disconnections with the metadata via NCBI e.g., species assignments we will use the NCBI data when in doubt. Given that NCBI confirm species assignments this seems a prudent choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "alberta_metadata = pd.read_csv('41598_2020_61002_MOESM2_ESM.csv', sep='\\t')\n",
    "\n",
    "# drop the 2 inexplicably duplicated rows \n",
    "alberta_metadata = alberta_metadata.drop_duplicates()\n",
    "\n",
    "# get rid of the identical rows apart from _ vs - \n",
    "alberta_metadata = alberta_metadata[alberta_metadata['ISOLATE'] != 'SWEntR-0393']\n",
    "# identical row apart from incorrect species (when compared to NCBI assembly)\n",
    "alberta_metadata = alberta_metadata[alberta_metadata['ISOLATE'] != 'SWEntR 0262']\n",
    "\n",
    "# tidy column names to help with merging later\n",
    "alberta_metadata = alberta_metadata.rename(columns={'ISOLATION SOURCE': 'Origin',\n",
    "                                                    'SPECIFIC LOCATION': 'Location',\n",
    "                                                    'VNCO': 'Vancomycin',\n",
    "                                                    'AMPI': 'Ampicillin',\n",
    "                                                    'TEIC': 'Teicoplanin',\n",
    "                                                    'DOXY': 'Doxycycline',\n",
    "                                                    'ERTH': 'Erythromycin',\n",
    "                                                    'GENT': 'Gentamicin',\n",
    "                                                    'LNZD': 'Linezolid',\n",
    "                                                    'LVFL': 'Levofloxacin',\n",
    "                                                    'QUIN': 'Quinolone',\n",
    "                                                    'STEP': 'Streptomycin',\n",
    "                                                    'NTRO': 'Nitrofurantoin',\n",
    "                                                    'TGC': 'Tigecycline',\n",
    "                                                    'SPECIATION': 'Species',\n",
    "                                                    'SOURCE CODE': 'Source_code',\n",
    "                                                    'ISOLATE': 'Isolate_name_paper'})\n",
    "\n",
    "# get rid of useless columns and add extra column to help with merging UK and AB metadata\n",
    "alberta_metadata = alberta_metadata.drop(['Unnamed: 18', \"Resistance count\", 'LOCATION'], axis=1)\n",
    "alberta_metadata['Country/Province'] = 'Canada/Alberta'\n",
    "\n",
    "# Remove trailing spaces that were left in the paper metadata\n",
    "alberta_metadata['Species'] = alberta_metadata['Species'].str.strip()\n",
    "\n",
    "# To ensure we merge the correct identifiers we are going to use the Source_code\n",
    "# information AS WELL as the isolate_name, therefore let's create a new identifier\n",
    "# out of the source code and isolate_name (and then remove spaces/underscores/hyphens etc)\n",
    "def combine_source_and_isolate_name(row):\n",
    "    \"\"\"\n",
    "    Try to combine isolate name with the source code field\n",
    "    if it isn't already prefixed by the source code information\n",
    "    \"\"\"\n",
    "    # Spaces, hyphens and dashes are a major source of disconnect so just remove them and try to map\n",
    "    metadata_isolate_name = row['Isolate_name_paper'].replace('_', '').replace('-', '').replace(' ', '')\n",
    "    if metadata_isolate_name.startswith(row['Source_code']):\n",
    "        return metadata_isolate_name\n",
    "    else:\n",
    "        return row['Source_code'] +  metadata_isolate_name\n",
    "\n",
    "\n",
    "alberta_metadata['Source_code_and_isolate_name'] = alberta_metadata.apply(\\\n",
    "                                                            combine_source_and_isolate_name, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-photograph",
   "metadata": {},
   "source": [
    "Now we want to merge in the accession data from the bioproject.\n",
    "\n",
    "The NCBI biosamples have both a distinct `Strain_name` and a distinct `Sample_name` whereas the paper metadata just has an `Isolate_name_paper` (original `ISOLATE`). \n",
    "\n",
    "`Isolate_name_paper` seems to correspond to either one of these NCBI identifiers with no apparent pattern. However, `Strain_name` does match the paper metadata `Isolate_name_paper` more often and seems to be generally closer.\n",
    "\n",
    "Therefore, we are going to try and identify mappings between the NCBI `Strain_name` and the paper metadata `Isolate_name_paper`.  Then we are going to treat the NCBI `Strain_name` as the true `Isolate_name`.\n",
    "\n",
    "Fortunately, it is usually fairly obvious what the correct mapping is as they usually only differ in hyphens/dashes/spacing/number of leading 0's. \n",
    "The `Strain_name` in NCBI often contain part of what is the `Source_code` in the paper metadata (i.e., source), therefore I combined `Source_code` with `Isolate_name_paper` as `Source_code_and_isolate_name` and then searched for mappings using that.   \n",
    "\n",
    "This means I can match `Strain_name` to `Source_code_and_isolate_name` by finding the `Source_code_and_isolate_name` that shares the longest suffix with each `Strain_name` in NCBI.\n",
    "\n",
    "For extra security to prevent mis-assignments, I also added the following filter conditions: \n",
    "\n",
    "1. All the numerical portions of `Strain_name` and `Source_code_and_isolate_name` had to match to be valid\n",
    "2. The relationship between the two sets of names had to be one-to-one i.e., each `Strain_name` was assigned to one and only one `Source_code_and_isolate_name`.\n",
    "\n",
    "Any remaining `Strain_name` in the NCBI data that wasn't assigned a `Source_code_and_isolate_name` can then be manually reviewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attempt_to_reconcile_name_sets(set1, set2):\n",
    "    \"\"\"\n",
    "    Try and find the closest match between strings in set1 to set2\n",
    "    First find the closest matching string by longest shared suffix\n",
    "    Then as a safety move extract all digits and confirm they match between\n",
    "    the biosample identifier and the closest paper identifier\n",
    "    \n",
    "    returns: dictionary mapping likely paper identifiers from set1 to set2\n",
    "    \"\"\"  \n",
    "    closest_match_strings_by_suffix_length = []\n",
    "    # for each name in set1 clean it up then and revese it (to make the suffix a prefix)\n",
    "    for set1_name in set1:\n",
    "        set1_name_clean = set1_name.replace('-', '').replace('_', '').lower()[::-1]\n",
    "        \n",
    "        # compare to each cleaned name in set2\n",
    "        distances_between_set1_name_and_all_set2 = []\n",
    "        longest_suffix = {'suffix_length': 0, 'set2_name': '', 'set1_name': set1_name}\n",
    "        for set2_name in set2:\n",
    "            set2_name_clean = set2_name.replace('-', '').replace('_', '').lower()[::-1]\n",
    "            \n",
    "            # and recover the string that has the longest shared suffix \n",
    "            suffix_length = 0\n",
    "            for set1_name_char, set2_name_char in zip(set1_name_clean, set2_name_clean):\n",
    "                if set1_name_char == set2_name_char:\n",
    "                    suffix_length += 1\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "            if suffix_length > longest_suffix['suffix_length']:\n",
    "                longest_suffix['suffix_length'] = suffix_length\n",
    "                longest_suffix['set2_name'] = set2_name\n",
    "\n",
    "            closest_match_strings_by_suffix_length.append(longest_suffix)\n",
    "    \n",
    "    # then check that the best matchs ALSO share all the same numerical components\n",
    "    closest_match_strings_by_suffix_size_and_numerical = []\n",
    "    for closest_pairing in closest_match_strings_by_suffix_length:\n",
    "        \n",
    "        # handle mapping long date names (failing due to numbers in the date portion)\n",
    "        if closest_pairing['set1_name'].startswith('ES-'):\n",
    "            set1_numerical = \"\".join(re.findall(r'\\d+', closest_pairing['set1_name'].split('-')[-1]))\n",
    "        else:\n",
    "            set1_numerical = \"\".join(re.findall(r'\\d+', closest_pairing['set1_name']))\n",
    "\n",
    "        if closest_pairing['set2_name'].startswith('ES-'):\n",
    "            set2_numerical =  \"\".join(re.findall(r'\\d+', closest_pairing['set2_name'].split('-')[-1]))\n",
    "        else:\n",
    "            set2_numerical =  \"\".join(re.findall(r'\\d+', closest_pairing['set2_name']))\n",
    "        \n",
    "        if set1_numerical == set2_numerical:\n",
    "            closest_match_strings_by_suffix_size_and_numerical.append((closest_pairing['set1_name'], \n",
    "                                                                       closest_pairing['set2_name']))\n",
    "        else:\n",
    "            # check if there is an issue with different numbers of leading 0s\n",
    "            # as this is a common issue\n",
    "            if set1_numerical.lstrip('0') == set2_numerical.lstrip('0'):\n",
    "                 closest_match_strings_by_suffix_size_and_numerical.append((closest_pairing['set1_name'], \n",
    "                                                                            closest_pairing['set2_name']))\n",
    "    \n",
    "    # create a dictionary from the closest hits and extract any in set1 without a mapping to set2\n",
    "    closest_match_strings_by_suffix_size_and_numerical = dict(closest_match_strings_by_suffix_size_and_numerical)\n",
    "    \n",
    "    # check for any non-one to one mappings in the dicionary i.e., different set1_name -> the same set2_name\n",
    "    # delete and manually resolve \n",
    "    set2_name_counter = Counter(closest_match_strings_by_suffix_size_and_numerical.values())\n",
    "    duplicates = []\n",
    "    closest_match_strings_by_suffix_size_and_numerical_no_duplicates = {}\n",
    "    for set1_name, set2_name in closest_match_strings_by_suffix_size_and_numerical.items():\n",
    "        # i.e. drop all those who don't have one-to-one mapping\n",
    "        if set2_name_counter[set2_name] == 1:\n",
    "            closest_match_strings_by_suffix_size_and_numerical_no_duplicates[set1_name] = set2_name\n",
    "    \n",
    "    set1_names_without_matches = set(set1) - set(closest_match_strings_by_suffix_size_and_numerical_no_duplicates.keys())\n",
    "    return closest_match_strings_by_suffix_size_and_numerical_no_duplicates, set1_names_without_matches\n",
    "\n",
    "alberta_ncbi_to_metadata_match, alberta_ncbi_without_metadata_match = attempt_to_reconcile_name_sets(alberta_ncbi['Strain_name'].values, \n",
    "                                                                                                    alberta_metadata['Source_code_and_isolate_name'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-medicare",
   "metadata": {},
   "source": [
    "Which NCBI strain_names didn't get a metadata match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "alberta_ncbi_without_metadata_match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-divide",
   "metadata": {},
   "source": [
    "Now we can try and manually fix these and identify cases of missing accessions or unresolvably ambiguous assignments (i.e., >1 perfectly valid appearing mapping from `Strain_name` to `Source_code_and_isolate_name` was possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = {'HC_NS0026',  # ambiguous \n",
    "           'HC_NS0078', # ambiguous\n",
    "           'HC_NS0854', # missing\n",
    "           'HC_NS1042', # missing\n",
    "           'HC_NS1090', # missing\n",
    "           'HC_NS1104', # missing\n",
    "           'HC_VRE0078', # missing\n",
    "           'HC_SS0026', #missing\n",
    "           'WW_0060M', # missing\n",
    "           'WW_0089I'} # missing\n",
    "\n",
    "manual_fixes = {'CB_0150': 'CBEntR0150',\n",
    "                 'CB_0182': 'CBEntR0182',\n",
    "                 'CB_0383': 'CBSWEntR0383',\n",
    "                 'ES-C-ST002-07DEC15-0142B': 'WW0142B',\n",
    "                 'FC_0142B': 'FC0142B',\n",
    "                 'HC_NS0150': 'NSSNS0150',\n",
    "                 'HC_NS0238': 'NSSNS0238',\n",
    "                 'HC_NS0383': 'NSSNS0383',\n",
    "                 'HC_NS210': 'NSSNS0210',\n",
    "                 'HC_SS0002': 'SS0002',\n",
    "                 'HC_SS0025': 'SS0025',\n",
    "                 'SW_0002': 'NWSEnt0002',\n",
    "                 'SW_0025': 'NWSEnt0025',\n",
    "                 'SW_0182': 'NWSEnt0182',\n",
    "                 'SW_0238': 'NWSEnt0238'}\n",
    "\n",
    "alberta_ncbi_to_metadata_match.update(manual_fixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-packaging",
   "metadata": {},
   "source": [
    "Unfortunately, this still leaves us with 10 isolates that are in the deposited NCBI data but don't seem to have any supplied metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "unresolved_ncbi = alberta_ncbi[~alberta_ncbi['Strain_name'].isin(alberta_ncbi_to_metadata_match.keys())].sort_values('Strain_name')['Strain_name'].values\n",
    "# add a status to the NCBI data indicating missing\n",
    "alberta_ncbi.loc[alberta_ncbi['Strain_name'].isin(unresolved_ncbi), 'Metadata_status'] = 'No metadata in original paper (10.1038/s41598-020-61002-5)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-permit",
   "metadata": {},
   "source": [
    "Using our mapping from `Strain_name` to `Source_code_and_isolate_name` let's add a `Source_code_and_isolate_name` to the NCBI metadata sheet and merge using that (ensuring valid one-to-one merging).  \n",
    "\n",
    "We are doing a `left` merge because we only want to keep the metadata from the paper for isolates that correspond to the NCBI bioproject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate the incorrect isolate_names in the paper metadata to the correct ones deposited in NCBI\n",
    "alberta_ncbi['Source_code_and_isolate_name'] = alberta_ncbi['Strain_name'].apply(lambda x: alberta_ncbi_to_metadata_match[x] if x in alberta_ncbi_to_metadata_match else f\"UNMATCHED: {x}\")\n",
    "\n",
    "# add the 10 unresolved identifiers to the paper metadata before merging\n",
    "alberta_metadata = pd.concat([alberta_metadata, pd.DataFrame({'Source_code_and_isolate_name': unresolved_ncbi})])\n",
    "\n",
    "alberta_merged = pd.merge(alberta_ncbi, alberta_metadata, \n",
    "                          validate='one_to_one', how='left', \n",
    "                          on='Source_code_and_isolate_name', suffixes=['_ncbi', '_paper'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-microphone",
   "metadata": {},
   "source": [
    "Finally, we want to tidy up the alberta metadata to make our ultimate merging with the UK data easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename NCBI species information as official species information\n",
    "# and change the NCBI `Strain_name` to `Isolate_name` to match better with other datasets (even though what\n",
    "# NCBI calls things should remain the master)\n",
    "alberta_merged = alberta_merged.rename(columns={'Species_ncbi': 'Species',\n",
    "                                                'Strain_name': 'Isolate_name'})\n",
    "\n",
    "# drop the superflous and likely erroneous species information from paper metadata sheet\n",
    "alberta_merged = alberta_merged.drop('Species_paper', axis=1)\n",
    "\n",
    "# reoroder the columns into logical groupings\n",
    "alberta_merged = alberta_merged[['Study_accession', 'Sample_accession', 'Isolate_name', 'Isolate_name_paper', 'Metadata_status',\n",
    "                                 'Species', 'Country/Province', \"Origin\", \"Location\", \"Source_code\", 'Ampicillin', 'Vancomycin', 'Teicoplanin',\n",
    "                                 'Doxycycline', 'Erythromycin', 'Nitrofurantoin',\n",
    "                                 'Gentamicin', 'Linezolid', 'Levofloxacin', 'Quinolone', 'Streptomycin',\n",
    "                                 'Tigecycline', 'Source_code_and_isolate_name']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-indianapolis",
   "metadata": {},
   "source": [
    "## Add read data to Alberta metadata\n",
    "\n",
    "Despite reconciling this at various stages the constant changes to metadata has meant this pairing has broken again, I'm applying the same approach as used for merging in the paper metadata to NCBI again. \n",
    "\n",
    "Specifically I need to link the \"Strain_name\" in `alberta_read_data` to the `Source_code_and_isolate_name` in the `alberta_merged` metadata dataframe\n",
    "\n",
    "Let's apply the same approach as last time: suffix + numerical matching and guaranteeing injectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_to_read_data, \\\n",
    "    metadata_to_read_data_mismatches = attempt_to_reconcile_name_sets(alberta_merged['Source_code_and_isolate_name'].dropna().values,\n",
    "                                                                      alberta_read_data['Strain_name'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-tamil",
   "metadata": {},
   "source": [
    "Now let's fix the mismatches that we can manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_to_read_data_mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_reads = {'BPC1383', # missing\n",
    "                 'BPH112E1', # missing\n",
    "                 'BPH532', # missing\n",
    "                 'CB0104A', # missing\n",
    "                 'CB0139J', # missing\n",
    "                 'CBEnt0179', # missing\n",
    "                 'FC0029A', # ambiguous\n",
    "                 'FC0064E', # missing\n",
    "                 'FC0142B', #ambiguous\n",
    "                 'FC0194A', # missing\n",
    "                 'FC0512A', # missing\n",
    "                 'FC0555C', # missing\n",
    "                 'FC0606I', # missing\n",
    "                 'FC0616B', # missing\n",
    "                 'FC0670I', # missing\n",
    "                 'FC0834B', # missing \n",
    "                 'FC0845J', # missing\n",
    "                 'NSSNS0176', # missing\n",
    "                 'NSSNS0210', # missing\n",
    "                 'NSSNS0554', # missing\n",
    "                 'NSSNS0564', # missing\n",
    "                 'NWSSWEnt0978', # missing\n",
    "                 'NWSSWEnt1013', # missing\n",
    "                 'NWSSWEnt1077', # missing\n",
    "                 'NWSSWEnt1143', # missing\n",
    "                 'NWSSWEntR0331', # missing   \n",
    "                 'SS0018', # missing\n",
    "                 'SS0030', # missing\n",
    "                 'VRE0030', # missing \n",
    "                 'VRE0067', # missing\n",
    "                 'VRE0069', # missing\n",
    "                 'VRE0084', # missing\n",
    "                 'VRE0090', # missing\n",
    "                 'WW0039J', # ambiguous\n",
    "                 'WW0050I', # missing\n",
    "                 'WW0050M', # missing\n",
    "                 'WW0081E', # missing\n",
    "                 'WW0124I', # missing\n",
    "                 'WW0137C', # missing\n",
    "                 'WW0141J', # missing\n",
    "                 'WW0141M', # missing\n",
    "                 'WW0142A', # missing\n",
    "                 'WW0142B', # missing\n",
    "                 'WW0142C', # missing \n",
    "                 'WW0142E', # missing\n",
    "                 'WW0142I', # missing\n",
    "                 'WW0146B'} # missing\n",
    "\n",
    "                 \n",
    "manual_fix_reads = {'NSSNS0331': 'NS0331',\n",
    "                    'VRE0018': 'VRE-0018',\n",
    "                    'WW0029A': 'ES-M-ST001-21JUL14-0029A'}\n",
    "\n",
    "\n",
    "metadata_to_read_data.update(manual_fix_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "alberta_merged.loc[~alberta_merged['Source_code_and_isolate_name'].isin(metadata_to_read_data.keys()), 'Read_status'] = \"Read data missing in dataset from HS\"\n",
    "alberta_merged['Isolate_name_reads'] = alberta_merged['Source_code_and_isolate_name'].apply(lambda x: metadata_to_read_data[x] if x in metadata_to_read_data else f\"UNMATCHED: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "alberta_read_data = alberta_read_data.rename(columns={'Strain_name': 'Isolate_name_reads'})\n",
    "\n",
    "alberta_data_all = pd.merge(alberta_merged, alberta_read_data, \n",
    "                          validate='one_to_one', how='left', \n",
    "                          on='Isolate_name_reads', suffixes=['_metadata', '_reads'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-original",
   "metadata": {},
   "source": [
    "Final tidy up before merging of UK and AAFC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally get rid of our franken colunm with the source code and the species names from the reads\n",
    "alberta_data_all = alberta_data_all.drop(['Source_code_and_isolate_name', 'Species_reads'], axis=1)\n",
    "\n",
    "# rename Species_metadata to Species again\n",
    "alberta_data_all = alberta_data_all.rename(columns={'Species_metadata': 'Species'})\n",
    "\n",
    "# finally reorder the columns into logical groupings to better understand the data (even though we have to do this again\n",
    "# after the big final merge)\n",
    "alberta_data_all = alberta_data_all[['Study_accession', 'Sample_accession', 'Isolate_name',\n",
    "                                     'Isolate_name_paper', 'Isolate_name_reads', \n",
    "                                     'Metadata_status', 'Species', 'Country/Province',\n",
    "                                     'Origin', 'Location', 'Source_code', 'Ampicillin', 'Vancomycin',\n",
    "                                     'Teicoplanin', 'Doxycycline', 'Erythromycin', 'Nitrofurantoin',\n",
    "                                     'Gentamicin', 'Linezolid', 'Levofloxacin', 'Quinolone', 'Streptomycin',\n",
    "                                     'Tigecycline', 'Read_status', 'Read_1', 'Read_2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-rotation",
   "metadata": {},
   "source": [
    "# Merging the two datasets\n",
    "\n",
    "Combine the two datasets and tidy the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([alberta_data_all, uk_metadata_and_reads])\n",
    "\n",
    "all_data = all_data[['Study_accession', 'Sample_accession', 'Run_accession', 'Isolate_name',\n",
    "                     'Isolate_name_paper', 'Isolate_name_reads', \n",
    "                     'Metadata_status', 'Species', 'BAPS_group','Sequence_Type', 'Country/Province', \n",
    "                     'Origin', 'Location', 'Source_code', \n",
    "                     'Ampicillin', 'Vancomycin', 'Teicoplanin', 'Doxycycline',\n",
    "                     'Erythromycin', 'Nitrofurantoin', 'Gentamicin', 'Linezolid',\n",
    "                     'Levofloxacin', 'Quinolone', 'Streptomycin', 'Tigecycline',\n",
    "                     'Read_status', 'Read_1', 'Read_2']]\n",
    "\n",
    "all_data.to_csv('all_combined_enterococcus_metadata.tsv', index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
